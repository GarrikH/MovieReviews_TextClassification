{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TF_Hub_Text_Classification_MovieReviews.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"20N15cKQzxpz","colab_type":"code","colab":{}},"source":["!pip install tensorflow==2.0.0\n","!pip install -q tensorflow-hub\n","!pip install -q tensorflow-datasets"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gAn7IC3myB6J","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","import numpy as np\n","import tensorflow as tf\n","\n","import tensorflow_hub as hub\n","import tensorflow_datasets as tfds\n","\n","print(\"Version:\", tf.__version__)\n","print(\"Eager Mode:\", tf.executing_eagerly())\n","print(\"Hub version:\", hub.__version__)\n","print(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT available\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"94EHYZ0G3hiB","colab_type":"text"},"source":["Split the training set into 60% and 40%, so we'll end up with 15,000 examples for training, 10,000 examples for validation and 25,000 examples for testing.\n"]},{"cell_type":"code","metadata":{"id":"Ewnts64c2Xxa","colab_type":"code","colab":{}},"source":["train_validation_split = tfds.Split.TRAIN.subsplit([6, 4])\n","\n","(train_data, validation_data) , test_data = tfds.load(\n","    name=\"imdb_reviews\",\n","    split=(train_validation_split, tfds.Split.TEST),\n","    as_supervised=True\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_aQZabHK4JX1","colab_type":"text"},"source":["Each example is a sentence representing the movie review and a corresponding label. The sentence is not preprocessed in any way. The label is an integer value of either 0 or 1, where 0 is a negative review, and 1 is a positive review."]},{"cell_type":"code","metadata":{"id":"plLV9BYM3Ymt","colab_type":"code","colab":{}},"source":["train_examples_batch, train_labels_batch = next(iter(train_data.batch(10)))\n","train_examples_batch"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"u3FaCIeo4Ca2","colab_type":"code","colab":{}},"source":["train_labels_batch"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zjSk_fVM5ipj","colab_type":"text"},"source":["# Build the model\n","The neural network is created by stacking layers—this requires three main architectural decisions:\n","\n","* How to represent the text?\n","* How many layers to use in the model?\n","* How many hidden units to use for each layer?\n","\n","In this example, the input data consists of sentences. The labels to predict are either 0 or 1.\n","\n","One way to represent the text is to convert sentences into embeddings vectors. We can use a pre-trained text embedding as the first layer, which will have three advantages: * we don't have to worry about text preprocessing, * we can benefit from transfer learning, * the embedding has a fixed size, so it's simpler to process.\n","\n","For this example we will use a pre-trained text embedding model from TensorFlow Hub called google/tf2-preview/gnews-swivel-20dim/1.\n","\n","There are three other pre-trained models to test for the sake of this tutorial:\n","\n","* google/tf2-preview/gnews-swivel-20dim-with-oov/1 - same as google/tf2-preview/gnews-swivel-20dim/1, but with 2.5% vocabulary converted to OOV buckets. This can help if vocabulary of the task and vocabulary of the model don't fully overlap.\n","* google/tf2-preview/nnlm-en-dim50/1 - A much larger model with ~1M vocabulary size and 50 dimensions.\n","* google/tf2-preview/nnlm-en-dim128/1 - Even larger model with ~1M vocabulary size and 128 dimensions.\n","\n","Let's first create a Keras layer that uses a TensorFlow Hub model to embed the sentences, and try it out on a couple of input examples. Note that no matter the length of the input text, the output shape of the embeddings is: `(num_examples, embedding_dimension)`."]},{"cell_type":"code","metadata":{"id":"SNyOR15m6AHn","colab_type":"code","colab":{}},"source":["embedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n","hub_layer = hub.KerasLayer(embedding, input_shape=[], dtype=tf.string, trainable=True)\n","hub_layer(train_examples_batch[:3])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_cSf7BR77Svo","colab_type":"text"},"source":["# Build the Model\n","\n","\n","1. The first layer is a TensorFlow Hub layer. This layer uses a pre-trained Saved Model to map a sentence into its embedding vector. The pre-trained text embedding model that we are using (google/tf2-preview/gnews-swivel-20dim/1) splits the sentence into tokens, embeds each token and then combines the embedding. The resulting dimensions are: `(num_examples, embedding_dimension)`.\n","2. This fixed-length output vector is piped through a fully-connected (Dense) layer with 16 hidden units.\n","3. The last layer is densely connected with a single output node. Using the sigmoid activation function, this value is a float between 0 and 1, representing a probability, or confidence level.\n"]},{"cell_type":"code","metadata":{"id":"T6_Ebegz60Bq","colab_type":"code","colab":{}},"source":["model = tf.keras.Sequential()\n","model.add(hub_layer)\n","model.add(tf.keras.layers.Dense(16, activation='relu'))\n","model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n","\n","model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CEW36TEN7rFu","colab_type":"text"},"source":["# Loss function and optimizer\n","A model needs a loss function and an optimizer for training. Since this is a binary classification problem and the model outputs a probability (a single-unit layer with a sigmoid activation), we'll use the `binary_crossentropy` loss function.\n","\n","This isn't the only choice for a loss function, you could, for instance, choose `mean_squared_error`. But, generally, `binary_crossentropy` is better for dealing with probabilities—it measures the \"distance\" between probability distributions, or in our case, between the ground-truth distribution and the predictions.\n","\n","Later, when we are exploring regression problems (say, to predict the price of a house), we will see how to use another loss function called mean squared error.\n","\n","Now, configure the model to use an optimizer and a loss function:"]},{"cell_type":"code","metadata":{"id":"_Ka3j2NX72hA","colab_type":"code","colab":{}},"source":["model.compile(\n","    optimizer='adam',\n","    loss='binary_crossentropy',\n","    metrics=['accuracy']\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4L7QqFH49JPc","colab_type":"code","colab":{}},"source":["history = model.fit(train_data.shuffle(10000).batch(512),\n","                    epochs=20,\n","                    validation_data=validation_data.batch(512),\n","                    verbose=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3gkjgUC1-Xf0","colab_type":"code","colab":{}},"source":["results = model.evaluate(test_data.batch(512), verbose=2)\n","for name, value in zip(model.metrics_names, results):\n","  print(\"%s: %.3f\" % (name, value))"],"execution_count":0,"outputs":[]}]}